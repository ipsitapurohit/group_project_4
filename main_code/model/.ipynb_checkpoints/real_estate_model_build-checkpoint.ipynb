{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888e5d2d",
   "metadata": {},
   "source": [
    "# Real Estate Stock Price Prediction - Data cleaning and preparation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "810abf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries for the project\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f94b8b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute path to database: C:\\Users\\ipsit\\OneDrive\\Documents\\GitHub\\real_estate_project\\main_code\\model\\florida_data.db\n"
     ]
    }
   ],
   "source": [
    "# db connection\n",
    "db_file = os.path.abspath('florida_data.db')\n",
    "engine = create_engine(f'sqlite:///{db_file}')\n",
    "\n",
    "# Print the absolute path to verify\n",
    "print(f\"Absolute path to database: {db_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aa97539",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(sqlite3.OperationalError) no such table: florida_data_cleaned\n[SQL: SELECT * FROM florida_data_cleaned]\n(Background on this error at: https://sqlalche.me/e/14/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1819\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1818\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1819\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mdo_execute(\n\u001b[0;32m   1820\u001b[0m             cursor, statement, parameters, context\n\u001b[0;32m   1821\u001b[0m         )\n\u001b[0;32m   1823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:732\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 732\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mexecute(statement, parameters)\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such table: florida_data_cleaned",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Establishing the connection with sqlite\u001b[39;00m\n\u001b[0;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mSELECT * FROM florida_data_cleaned\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m----> 3\u001b[0m re_df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(query, engine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:663\u001b[0m, in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    654\u001b[0m         sql,\n\u001b[0;32m    655\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    660\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    661\u001b[0m     )\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_query(\n\u001b[0;32m    664\u001b[0m         sql,\n\u001b[0;32m    665\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m    666\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    667\u001b[0m         coerce_float\u001b[38;5;241m=\u001b[39mcoerce_float,\n\u001b[0;32m    668\u001b[0m         parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[0;32m    669\u001b[0m         chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    670\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    671\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    672\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:1738\u001b[0m, in \u001b[0;36mSQLDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   1681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[0;32m   1682\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1683\u001b[0m     sql: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1690\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1691\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[0;32m   1692\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1693\u001b[0m \u001b[38;5;124;03m    Read SQL query into a DataFrame.\u001b[39;00m\n\u001b[0;32m   1694\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1736\u001b[0m \n\u001b[0;32m   1737\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1738\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(sql, params)\n\u001b[0;32m   1739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m   1741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:1562\u001b[0m, in \u001b[0;36mSQLDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   1560\u001b[0m args \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m [params]\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sql, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcon\u001b[38;5;241m.\u001b[39mexec_driver_sql(sql, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcon\u001b[38;5;241m.\u001b[39mexecute(sql, \u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1686\u001b[0m, in \u001b[0;36mConnection.exec_driver_sql\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1636\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Executes a SQL statement construct and returns a\u001b[39;00m\n\u001b[0;32m   1637\u001b[0m \u001b[38;5;124;03m:class:`_engine.CursorResult`.\u001b[39;00m\n\u001b[0;32m   1638\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1681\u001b[0m \n\u001b[0;32m   1682\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1684\u001b[0m args_10style, kwargs_10style \u001b[38;5;241m=\u001b[39m _distill_params_20(parameters)\n\u001b[1;32m-> 1686\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exec_driver_sql(\n\u001b[0;32m   1687\u001b[0m     statement,\n\u001b[0;32m   1688\u001b[0m     args_10style,\n\u001b[0;32m   1689\u001b[0m     kwargs_10style,\n\u001b[0;32m   1690\u001b[0m     execution_options,\n\u001b[0;32m   1691\u001b[0m     future\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1692\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1595\u001b[0m, in \u001b[0;36mConnection._exec_driver_sql\u001b[1;34m(self, statement, multiparams, params, execution_options, future)\u001b[0m\n\u001b[0;32m   1585\u001b[0m         (\n\u001b[0;32m   1586\u001b[0m             statement,\n\u001b[0;32m   1587\u001b[0m             distilled_params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1591\u001b[0m             statement, distilled_parameters, execution_options\n\u001b[0;32m   1592\u001b[0m         )\n\u001b[0;32m   1594\u001b[0m dialect \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\n\u001b[1;32m-> 1595\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_context(\n\u001b[0;32m   1596\u001b[0m     dialect,\n\u001b[0;32m   1597\u001b[0m     dialect\u001b[38;5;241m.\u001b[39mexecution_ctx_cls\u001b[38;5;241m.\u001b[39m_init_statement,\n\u001b[0;32m   1598\u001b[0m     statement,\n\u001b[0;32m   1599\u001b[0m     distilled_parameters,\n\u001b[0;32m   1600\u001b[0m     execution_options,\n\u001b[0;32m   1601\u001b[0m     statement,\n\u001b[0;32m   1602\u001b[0m     distilled_parameters,\n\u001b[0;32m   1603\u001b[0m )\n\u001b[0;32m   1605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m future:\n\u001b[0;32m   1606\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1862\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1859\u001b[0m             branched\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1862\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_dbapi_exception(\n\u001b[0;32m   1863\u001b[0m         e, statement, parameters, cursor, context\n\u001b[0;32m   1864\u001b[0m     )\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2043\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[0;32m   2041\u001b[0m     util\u001b[38;5;241m.\u001b[39mraise_(newraise, with_traceback\u001b[38;5;241m=\u001b[39mexc_info[\u001b[38;5;241m2\u001b[39m], from_\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m   2042\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[1;32m-> 2043\u001b[0m     util\u001b[38;5;241m.\u001b[39mraise_(\n\u001b[0;32m   2044\u001b[0m         sqlalchemy_exception, with_traceback\u001b[38;5;241m=\u001b[39mexc_info[\u001b[38;5;241m2\u001b[39m], from_\u001b[38;5;241m=\u001b[39me\n\u001b[0;32m   2045\u001b[0m     )\n\u001b[0;32m   2046\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2047\u001b[0m     util\u001b[38;5;241m.\u001b[39mraise_(exc_info[\u001b[38;5;241m1\u001b[39m], with_traceback\u001b[38;5;241m=\u001b[39mexc_info[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\compat.py:208\u001b[0m, in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    205\u001b[0m     exception\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m replace_context\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# credit to\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# as the __traceback__ object creates a cycle\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m exception, replace_context, from_, with_traceback\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1819\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1817\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1818\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1819\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mdo_execute(\n\u001b[0;32m   1820\u001b[0m             cursor, statement, parameters, context\n\u001b[0;32m   1821\u001b[0m         )\n\u001b[0;32m   1823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[0;32m   1824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_cursor_execute(\n\u001b[0;32m   1825\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1826\u001b[0m         cursor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1830\u001b[0m         context\u001b[38;5;241m.\u001b[39mexecutemany,\n\u001b[0;32m   1831\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:732\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 732\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mexecute(statement, parameters)\n",
      "\u001b[1;31mOperationalError\u001b[0m: (sqlite3.OperationalError) no such table: florida_data_cleaned\n[SQL: SELECT * FROM florida_data_cleaned]\n(Background on this error at: https://sqlalche.me/e/14/e3q8)"
     ]
    }
   ],
   "source": [
    "# Establishing the connection with sqlite\n",
    "query = '''SELECT * FROM florida_data_cleaned'''\n",
    "re_df1 = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85bf089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few records\n",
    "re_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8068a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of rows and columns\n",
    "re_df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc817c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop columns that are not needed for data analysis\n",
    "re_df2 = re_df1.drop(['state','zip_code'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489cad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few records of updated dataframe\n",
    "re_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1222f286",
   "metadata": {},
   "source": [
    "# Data cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842cdbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find count of rows with null values for each of the category\n",
    "re_df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0848952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows with null values\n",
    "re_df_clean = re_df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf111d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any rows have null values now\n",
    "re_df_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96083c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display few of the rows of the updated dataframe\n",
    "re_df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7bf3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of rows and cloumns of the updated dataframe\n",
    "re_df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f24310",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b86f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_df_clean['bed'] = re_df_clean['bed'].astype(float)\n",
    "re_df_clean['bath'] = re_df_clean['bath'].astype(float)\n",
    "re_df_clean['price'] = re_df_clean['price'].astype(float)\n",
    "re_df_clean['house_size'] = re_df_clean['house_size'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440bbde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b277b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of rows with number of bedrooms > 10\n",
    "re_df_clean[re_df_clean.bed>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e9d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with bed size > 10 removed\n",
    "re_df_cleaned = re_df_clean[re_df_clean['bed'] <= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e411a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some of the rows of the cleaned dataset\n",
    "re_df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c1ee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of rows and columns of the updated dataframe\n",
    "re_df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5f77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any rows with number of beds > 10\n",
    "re_df_cleaned[re_df_cleaned.bed>10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3234af",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e4db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy dataframe into new dataframe to create an additional column i.e. price_per_sqft\n",
    "re_df_cleaned2 = re_df_cleaned.copy()\n",
    "re_df_cleaned2['price_per_sqft'] = (re_df_cleaned['price'] / re_df_cleaned['house_size']).round(2)\n",
    "re_df_cleaned2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab9a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique count of cities\n",
    "len(re_df_cleaned2.city.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f9e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up city column values by removing leading and trailing spaces\n",
    "re_df_cleaned2.city = re_df_cleaned2.city.apply(lambda x: x.strip())\n",
    "\n",
    "# Find and display  the count of rows per city\n",
    "city_stats = re_df_cleaned2.groupby('city')['city'].agg('count').sort_values(ascending = False)\n",
    "city_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17adbfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out count of cities with less than 10 data points\n",
    "len(city_stats[city_stats<=10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7689e35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display cities with less than 10 data points\n",
    "city_stats_less_than_10 = city_stats[city_stats<=10]\n",
    "city_stats_less_than_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14bb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all the cities with less than 10 data points with \"other\"\n",
    "re_df_cleaned2.city = re_df_cleaned2.city.apply(lambda x: 'other' if x in city_stats_less_than_10 else x)\n",
    "len(re_df_cleaned2.city.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf1e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_df_cleaned2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213b6439",
   "metadata": {},
   "source": [
    "# Outlier detection and removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfebabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out the data points where total square feet of the house divided by number of bedrooms is less than 300\n",
    "re_df_cleaned2[(re_df_cleaned2.house_size/re_df_cleaned2.bed)<300].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793bb54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_df_cleaned2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a621e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove above outliers\n",
    "re_df_cleaned3 = re_df_cleaned2[~((re_df_cleaned2.house_size/re_df_cleaned2.bed)<300)]\n",
    "re_df_cleaned3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d2584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extremely low and extremely high price per sqft columns\n",
    "re_df_cleaned3.price_per_sqft.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954229f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write function to remove outliers  \n",
    "\n",
    "def remove_ppsqft_outliers(df):\n",
    "    df_out = pd.DataFrame()\n",
    "    for key, subdf in df.groupby('city'):\n",
    "        m = np.mean(subdf.price_per_sqft)\n",
    "        st = np.std(subdf.price_per_sqft)\n",
    "        reduced_df = subdf[(subdf.price_per_sqft>(m-st)) & (subdf.price_per_sqft<=(m+st))]\n",
    "        df_out = pd.concat([df_out,reduced_df],ignore_index=True)\n",
    "    return df_out\n",
    "\n",
    "re_df_cleaned4 = remove_ppsqft_outliers(re_df_cleaned3)\n",
    "re_df_cleaned4.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dc12ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram with price per square feet and count of data points\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (20,10)\n",
    "plt.hist(re_df_cleaned4.price_per_sqft)\n",
    "plt.xlabel(\"Price per square feet\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram with number of bathrooms and count of data points\n",
    "plt.hist(re_df_cleaned4.bath)\n",
    "plt.xlabel(\"Number of bathrooms\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d26843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out data points where number of bathrooms is greater than bedrooms + 2\n",
    "re_df_cleaned4[re_df_cleaned4.bath>re_df_cleaned4.bed+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing data points where number of bathrooms is greater than bedrooms + 2\n",
    "re_df_cleaned5 = re_df_cleaned4[~(re_df_cleaned4.bath>re_df_cleaned4.bed+2)]\n",
    "re_df_cleaned5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f9c033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop price_per_sqft,zip_code columns\n",
    "re_df_cleaned6 = re_df_cleaned5.drop(columns=['price_per_sqft'])\n",
    "re_df_cleaned6.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e744c02e",
   "metadata": {},
   "source": [
    "# Build Machine Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78119282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'city' column is of string type\n",
    "re_df_cleaned6['city'] = re_df_cleaned6['city'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e3832",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dummy variables for the 'city' column\n",
    "city_dummies = pd.get_dummies(re_df_cleaned6['city'])\n",
    "\n",
    "# Ensure the dummy variables are 0 and 1\n",
    "city_dummies = city_dummies.astype(int)\n",
    "\n",
    "# Print dummy variables\n",
    "city_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458cc45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping 'other' column\n",
    "re_df_cleaned7 = pd.concat([re_df_cleaned6,city_dummies.drop('other',axis='columns')],axis='columns')\n",
    "re_df_cleaned7.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e9cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping city column\n",
    "re_df_cleaned8 = re_df_cleaned7.drop('city',axis='columns')\n",
    "re_df_cleaned8.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e40365",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_df_cleaned8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3b301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x and y axis for the dataset\n",
    "x = re_df_cleaned8.drop('price',axis='columns')\n",
    "x.shape\n",
    "\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3529029",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = re_df_cleaned8.price\n",
    "y.shape\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9688a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train_test_split function from scikit-learn's model_selection module\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets with 80% data used for training and 20% used for testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a248697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LinearRegression class from scikit-learn's linear_model module\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create an instance of the LinearRegression class, which represents the linear regression model\n",
    "lr_clf = LinearRegression()\n",
    "\n",
    "# Train the linear regression model using the training data\n",
    "lr_clf.fit(x_train,y_train)\n",
    "\n",
    "# Evaluate the model's performance using the testing data and return the coefficient of determination (R^2 score)\n",
    "lr_clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4617cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from scikit-learn\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "# ShuffleSplit will randomly split the data into training and test sets\n",
    "# n_splits=5 means the data will be split 5 times\n",
    "# test_size=0.2 means 20% of the data will be used as the test set in each split\n",
    "# random_state=0 ensures reproducibility of the splits\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "\n",
    "# Perform cross-validation using Linear Regression as the estimator\n",
    "# cross_val_score returns the accuracy scores for each split\n",
    "# x is the input features and y is the target variable\n",
    "scores = cross_val_score(LinearRegression(), x, y, cv=cv)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca75d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515315cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_home_price(location, bed, bath, sqft):    \n",
    "    # Check if city exists in the columns\n",
    "    if location in x.columns:\n",
    "        loc_index = np.where(x.columns == location)[0][0]\n",
    "    else:\n",
    "        loc_index = -1  # Set to -1 if city is not found\n",
    "    \n",
    "    # Create a zero array of the same length as the number of columns in x\n",
    "    x1 = np.zeros(len(x.columns))\n",
    "    \n",
    "    # Assign the input features to the appropriate positions in the array\n",
    "    x1[0] = bed\n",
    "    x1[1] = bath\n",
    "    x1[2] = sqft\n",
    "    \n",
    "    # If the city is found, set its position to 1\n",
    "    if loc_index >= 0:\n",
    "        x1[loc_index] = 1\n",
    "    \n",
    "    # Reshape x1 to a 2D array (1 sample, many features)\n",
    "    x1 = x1.reshape(1, -1)\n",
    "    \n",
    "    # Return the predicted price using the trained model\n",
    "    return lr_clf.predict(x1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8cec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the model is working\n",
    "predict_home_price('Wimauma', 3, 2, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830fd153",
   "metadata": {},
   "source": [
    "# Export the tested model to a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae07f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./back-end/artifacts/florida_home_prices_model.pickle','wb') as f:\n",
    "    pickle.dump(lr_clf,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b4daa",
   "metadata": {},
   "source": [
    "# Export location and column information to a file that will be useful later on in our prediction application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eaa426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "columns = {\n",
    "    'data_columns' : [col.lower() for col in x.columns]\n",
    "}\n",
    "with open(\"./back-end/artifacts/columns.json\",\"w\") as f:\n",
    "    f.write(json.dumps(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48254f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e325265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
